{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install chroma-hnswlib==0.7.3\n",
        "!pip install chromadb==0.4.24\n",
        "!pip install llama-index\n",
        "!pip install llama_index.llms.huggingface\n",
        "!pip install llama_index.embeddings.huggingface\n",
        "!pip install llama_index.vector_stores.chroma"
      ],
      "metadata": {
        "id": "sVIfOHlp856R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfao6XBA8tND"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, Settings\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import ChatPromptTemplate\n",
        "import chromadb\n",
        "import os\n",
        "\n",
        "\n",
        "Settings.llm = HuggingFaceInferenceAPI(\n",
        "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    context_window=3900,\n",
        "    token='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',\n",
        "    max_new_tokens=1000,\n",
        "    generate_kwargs={\"temperature\": 0},\n",
        ")\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 512\n",
        "Settings.chunk_overlap = 64\n",
        "\n",
        "\n",
        "KNOWLEDGE_BASE_DIR = \"Knowledge_base\"\n",
        "CHROMA_DB_PATH = \"./chroma_db\"\n",
        "\n",
        "\n",
        "os.makedirs(KNOWLEDGE_BASE_DIR, exist_ok=True)\n",
        "\n",
        "def data_ingestion():\n",
        "    documents = SimpleDirectoryReader(KNOWLEDGE_BASE_DIR).load_data()\n",
        "    splitter = SentenceSplitter()\n",
        "\n",
        "    db = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
        "    chroma_collection = db.get_or_create_collection(\"DB_collection\")\n",
        "\n",
        "\n",
        "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "\n",
        "    VectorStoreIndex.from_documents(documents, storage_context=storage_context, embed_model=embed_model, transformations=[splitter])\n",
        "\n",
        "def handle_query(query):\n",
        "    db = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
        "    chroma_collection = db.get_or_create_collection(\"DB_collection\")\n",
        "\n",
        "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "    index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)\n",
        "\n",
        "    chat_text_qa_msgs = [\n",
        "        (\n",
        "            \"user\",\n",
        "            \"\"\"You are a professional Questiona and Answer based chatbot, developed by an ML Engineer named Shashank Agasimani. Your main goal is to provide answers as accurately as possible, only based on the instructions and context you have been given. If a question does not match the provided context or is outside the scope of the document, you only say the user to 'Please ask a questions within the context of the document'.\n",
        "            Context:\n",
        "            {context_str}\n",
        "            Question:\n",
        "            {query_str}\n",
        "            \"\"\"\n",
        "        )\n",
        "    ]\n",
        "    text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n",
        "    query_engine = index.as_query_engine(text_qa_template=text_qa_template)\n",
        "    answer = query_engine.query(query)\n",
        "\n",
        "    if hasattr(answer, 'response'):\n",
        "        return answer.response\n",
        "    elif isinstance(answer, dict) and 'response' in answer:\n",
        "        return answer['response']\n",
        "    else:\n",
        "        return \"Sorry, I couldn't find an answer.\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    data_ingestion()\n",
        "\n",
        "\n",
        "    query = \"Max Free Air of PG 600S-200\"\n",
        "    response = handle_query(query)\n",
        "    print(response)\n",
        "\n"
      ]
    }
  ]
}